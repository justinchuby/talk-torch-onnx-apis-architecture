# PyTorch ONNX Conversion Report

```
✅ Obtain model graph with `torch.export.export(..., strict=False)`
⚪ Obtain model graph with `torch.export.export(..., strict=True)`
⚪ Obtain model graph with `torch.export.draft_export`
✅ Decompose operators for ONNX compatibility
✅ Translate the graph into ONNX
⚪ Run `onnx.checker` on the ONNX model
⚪ Execute the model with ONNX Runtime
⚪ Validate model output accuracy
```

## Error messages

```pytb
No errors
```

## Exported program

```python
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_weight: "f32[1, 3, 1, 10]", x: "f32[2, 3, 10, 10]", y: "f32[2, 3, 10, 10]"):
             # File: /tmp/ipykernel_246584/911034161.py:10 in forward, code: a = torch.sin(x)
            sin: "f32[2, 3, 10, 10]" = torch.ops.aten.sin.default(x);  x = None
            
             # File: /tmp/ipykernel_246584/911034161.py:11 in forward, code: a.add_(y)
            add: "f32[2, 3, 10, 10]" = torch.ops.aten.add.Tensor(sin, y);  sin = y = None
            
             # File: /tmp/ipykernel_246584/911034161.py:12 in forward, code: b = a * self.weight
            mul: "f32[2, 3, 10, 10]" = torch.ops.aten.mul.Tensor(add, p_weight);  add = p_weight = None
            
             # File: /tmp/ipykernel_246584/911034161.py:13 in forward, code: return torch.nn.functional.scaled_dot_product_attention(b, b, b)
            scaled_dot_product_attention: "f32[2, 3, 10, 10]" = torch.ops.aten.scaled_dot_product_attention.default(mul, mul, mul);  mul = None
            return (scaled_dot_product_attention,)
            
Graph signature: 
    # inputs
    p_weight: PARAMETER target='weight'
    x: USER_INPUT
    y: USER_INPUT
    
    # outputs
    scaled_dot_product_attention: USER_OUTPUT
    
Range constraints: {}

```

## ONNX model

```python
<
    ir_version=10,
    opset_imports={'': 18},
    producer_name='pytorch',
    producer_version='2.10.0.dev20251028+cpu',
    domain=None,
    model_version=None,
>
graph(
    name=main_graph,
    inputs=(
        %"x"<FLOAT,[2,3,10,10]>,
        %"y"<FLOAT,[2,3,10,10]>
    ),
    outputs=(
        %"scaled_dot_product_attention"<FLOAT,[2,3,10,10]>
    ),
    initializers=(
        %"weight"<FLOAT,[1,3,1,10]>{TorchTensor(...)}
    ),
) {
    0 |  # node_sin
         %"sin"<FLOAT,[2,3,10,10]> ⬅️ ::Sin(%"x")
    1 |  # node_add
         %"add"<FLOAT,[2,3,10,10]> ⬅️ ::Add(%"sin", %"y")
    2 |  # node_mul
         %"mul"<FLOAT,[2,3,10,10]> ⬅️ ::Mul(%"add", %"weight"{...})
    3 |  # node_scaled_dot_product_attention
         %"scaled_dot_product_attention"<FLOAT,[2,3,10,10]>, %"val_0"<?,?>, %"val_1"<?,?>, %"val_2"<?,?> ⬅️ ::Attention(%"mul", %"mul", %"mul") {qk_matmul_output_mode=0, softcap=0.0, is_causal=0}
    return %"scaled_dot_product_attention"<FLOAT,[2,3,10,10]>
}


```

## Analysis

PyTorch ONNX Conversion Analysis

## Model Information

The model has 30 parameters and 0 buffers (non-trainable parameters).
Number of parameters per dtype:
```python
defaultdict(<class 'int'>, {torch.float32: 30})
```
Number of buffers per dtype:
```python
defaultdict(<class 'int'>, {})
```

Inputs:
- `x`: `TensorMetadata(shape=torch.Size([2, 3, 10, 10]), dtype=torch.float32, requires_grad=False, stride=(300, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})`
- `y`: `TensorMetadata(shape=torch.Size([2, 3, 10, 10]), dtype=torch.float32, requires_grad=False, stride=(300, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})`

Outputs:
- `scaled_dot_product_attention`: `TensorMetadata(shape=torch.Size([2, 3, 10, 10]), dtype=torch.float32, requires_grad=False, stride=(300, 100, 10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})`

The FX graph has 8 nodes in total. Number of FX nodes per op:
- `placeholder`: 3
- `call_function`: 4
- `output`: 1


Of the call_function nodes, the counts of operators used are:

- `aten.sin.default`: 1
- `aten.add.Tensor`: 1
- `aten.mul.Tensor`: 1
- `aten.scaled_dot_product_attention.default`: 1

## ONNX Conversion Information

All operators in the model have registered ONNX decompositions.

## Decomposition comparison

Ops exist only in the ExportedProgram before decomposition: `['aten.add_.Tensor']`

Ops exist only in the ExportedProgram after decomposition: `['aten.add.Tensor']`

