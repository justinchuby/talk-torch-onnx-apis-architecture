# PyTorch ONNX Conversion Report

```
✅ Obtain model graph with `torch.export.export(..., strict=False)`
⚪ Obtain model graph with `torch.export.export(..., strict=True)`
⚪ Obtain model graph with `torch.export.draft_export`
✅ Decompose operators for ONNX compatibility
✅ Translate the graph into ONNX
✅ Run `onnx.checker` on the ONNX model
✅ Execute the model with ONNX Runtime
✅ Validate model output accuracy
```

## Error messages

```pytb
No errors
```

## Exported program

```python
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, x: "f32[10, 10]", y: "f32[10, 10]"):
             # File: /tmp/ipykernel_246584/1981893269.py:6 in forward, code: a = torch.sin(x)
            sin: "f32[10, 10]" = torch.ops.aten.sin.default(x);  x = None
            
             # File: /tmp/ipykernel_246584/1981893269.py:7 in forward, code: a.add_(y)
            add: "f32[10, 10]" = torch.ops.aten.add.Tensor(sin, y);  sin = y = None
            return (add,)
            
Graph signature: 
    # inputs
    x: USER_INPUT
    y: USER_INPUT
    
    # outputs
    add: USER_OUTPUT
    
Range constraints: {}

```

## ONNX model

```python
<
    ir_version=10,
    opset_imports={'': 18},
    producer_name='pytorch',
    producer_version='2.10.0.dev20251028+cpu',
    domain=None,
    model_version=None,
>
graph(
    name=main_graph,
    inputs=(
        %"x"<FLOAT,[10,10]>,
        %"y"<FLOAT,[10,10]>
    ),
    outputs=(
        %"add"<FLOAT,[10,10]>
    ),
) {
    0 |  # node_sin
         %"sin"<FLOAT,[10,10]> ⬅️ ::Sin(%"x")
    1 |  # node_add
         %"add"<FLOAT,[10,10]> ⬅️ ::Add(%"sin", %"y")
    return %"add"<FLOAT,[10,10]>
}


```

## Analysis

PyTorch ONNX Conversion Analysis

## Model Information

The model has 0 parameters and 0 buffers (non-trainable parameters).
Number of parameters per dtype:
```python
defaultdict(<class 'int'>, {})
```
Number of buffers per dtype:
```python
defaultdict(<class 'int'>, {})
```

Inputs:
- `x`: `TensorMetadata(shape=torch.Size([10, 10]), dtype=torch.float32, requires_grad=False, stride=(10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})`
- `y`: `TensorMetadata(shape=torch.Size([10, 10]), dtype=torch.float32, requires_grad=False, stride=(10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})`

Outputs:
- `add`: `TensorMetadata(shape=torch.Size([10, 10]), dtype=torch.float32, requires_grad=False, stride=(10, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})`

The FX graph has 5 nodes in total. Number of FX nodes per op:
- `placeholder`: 2
- `call_function`: 2
- `output`: 1


Of the call_function nodes, the counts of operators used are:

- `aten.sin.default`: 1
- `aten.add.Tensor`: 1

## ONNX Conversion Information

All operators in the model have registered ONNX decompositions.

## Decomposition comparison

Ops exist only in the ExportedProgram before decomposition: `['aten.add_.Tensor']`

Ops exist only in the ExportedProgram after decomposition: `['aten.add.Tensor']`


## Verification results

`add`: `max_abs_diff=1.192093e-07`, `max_rel_diff=2.373025e-07`, `abs_diff_hist=torch.return_types.histogram(
hist=tensor([100.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]),
bin_edges=tensor([0.0000e+00, 1.0000e-06, 1.0000e-05, 1.0000e-04, 1.0000e-03, 1.0000e-02,
        1.0000e-01, 1.0000e+00, 1.0000e+01, 1.0000e+06]))`, `rel_diff_hist=torch.return_types.histogram(
hist=tensor([100.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]),
bin_edges=tensor([0.0000e+00, 1.0000e-06, 1.0000e-05, 1.0000e-04, 1.0000e-03, 1.0000e-02,
        1.0000e-01, 1.0000e+00, 1.0000e+01, 1.0000e+06]))`
