{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e012217",
   "metadata": {},
   "source": [
    "# PyTorch ONNX Exporter new features and architecture\n",
    "\n",
    "Jan 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c8b41c",
   "metadata": {},
   "source": [
    "![infographics](<pt-onnx-infographics.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ee4cda",
   "metadata": {},
   "source": [
    "## `dynamo=True` is the default\n",
    "\n",
    "- The New Default: Starting from PyTorch 2.9, the **`dynamo=True`** option is the **default and recommended** way to export models to ONNX.\n",
    "- Core Shift: It moves away from the older TorchScript-based capture mechanism to a torch.export based modern stack.\n",
    "- Deprecation Plan: While the TorchScript exporter (dynamo=False) is currently usable, it is planned for eventual deprecation in alignment with PyTorch core's handling of TorchScript."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f5a6f",
   "metadata": {},
   "source": [
    "## New options in `export()`\n",
    "\n",
    "```py\n",
    "torch.onnx.export(\n",
    "    model, args, kwargs=kwargs,\n",
    "    # New way of expressing dynamic shapes (more examples later)\n",
    "    dynamic_shapes=({0: \"batch\", 1: \"sequence_len\"}),\n",
    "    # dynamic_axes=...,  # Deprecated\n",
    "    dynamo=True,  # Default (2.9)\n",
    "    report=True,  # Creates a markdown report\n",
    "    verify=True,  # Runs onnx runtime on the example\n",
    "    optimize=True, # Runs onnxscript graph optimizations\n",
    ") -> torch.onnx.ONNXProgram\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f989ac",
   "metadata": {},
   "source": [
    "## What happens inside `torch.onnx.export`\n",
    "\n",
    "torch.export() **captures FX** graph\n",
    "-> **translate** and build ONNX IR\n",
    "-> graph **optimization** with ONNX Script\n",
    "\n",
    "Entry point is at: https://github.com/pytorch/pytorch/blob/0ad306cac740eaf2ce582e2bdf097cc61d929a40/torch/onnx/_internal/exporter/_core.py#L1282\n",
    "\n",
    "![diagram](https://raw.githubusercontent.com/justinchuby/diagrams/refs/heads/main/pytorch/torch-export-flow.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FX graph and the ExportedProgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dde3c91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExportedProgram:\n",
      "    class GraphModule(torch.nn.Module):\n",
      "        def forward(self, p_weight: \"f32[1, 3, 1, 10]\", x: \"f32[2, 3, 10, 10]\", y: \"f32[2, 3, 10, 10]\"):\n",
      "             # File: /tmp/ipykernel_246584/911034161.py:10 in forward, code: a = torch.sin(x)\n",
      "            sin: \"f32[2, 3, 10, 10]\" = torch.ops.aten.sin.default(x);  x = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_246584/911034161.py:11 in forward, code: a.add_(y)\n",
      "            add_: \"f32[2, 3, 10, 10]\" = torch.ops.aten.add_.Tensor(sin, y);  sin = y = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_246584/911034161.py:12 in forward, code: b = a * self.weight\n",
      "            mul: \"f32[2, 3, 10, 10]\" = torch.ops.aten.mul.Tensor(add_, p_weight);  add_ = p_weight = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_246584/911034161.py:13 in forward, code: return torch.nn.functional.scaled_dot_product_attention(b, b, b)\n",
      "            scaled_dot_product_attention: \"f32[2, 3, 10, 10]\" = torch.ops.aten.scaled_dot_product_attention.default(mul, mul, mul);  mul = None\n",
      "            return (scaled_dot_product_attention,)\n",
      "            \n",
      "Graph signature: \n",
      "    # inputs\n",
      "    p_weight: PARAMETER target='weight'\n",
      "    x: USER_INPUT\n",
      "    y: USER_INPUT\n",
      "    \n",
      "    # outputs\n",
      "    scaled_dot_product_attention: USER_OUTPUT\n",
      "    \n",
      "Range constraints: {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.export\n",
    "\n",
    "class Mod(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.randn(1, 3, 1, 10))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        a = torch.sin(x)\n",
    "        a.add_(y)\n",
    "        b = a * self.weight\n",
    "        return torch.nn.functional.scaled_dot_product_attention(b, b, b)\n",
    "\n",
    "example_args = (torch.randn(2, 3, 10, 10), torch.randn(2, 3, 10, 10))\n",
    "\n",
    "# Important to set to eval mode before exporting\n",
    "mod = Mod().eval()\n",
    "exported_program: \"ExportedProgram\" = torch.export.export(mod, args=example_args)\n",
    "print(exported_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5dd8edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExportedProgram:\n",
      "    class GraphModule(torch.nn.Module):\n",
      "        def forward(self, p_weight: \"f32[1, 3, 1, 10]\", x: \"f32[2, 3, 10, 10]\", y: \"f32[2, 3, 10, 10]\"):\n",
      "             # File: /tmp/ipykernel_246584/911034161.py:10 in forward, code: a = torch.sin(x)\n",
      "            sin: \"f32[2, 3, 10, 10]\" = torch.ops.aten.sin.default(x);  x = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_246584/911034161.py:11 in forward, code: a.add_(y)\n",
      "            add: \"f32[2, 3, 10, 10]\" = torch.ops.aten.add.Tensor(sin, y);  sin = y = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_246584/911034161.py:12 in forward, code: b = a * self.weight\n",
      "            mul: \"f32[2, 3, 10, 10]\" = torch.ops.aten.mul.Tensor(add, p_weight);  add = p_weight = None\n",
      "            \n",
      "             # File: /tmp/ipykernel_246584/911034161.py:13 in forward, code: return torch.nn.functional.scaled_dot_product_attention(b, b, b)\n",
      "            mul_1: \"f32[2, 3, 10, 10]\" = torch.ops.aten.mul.Scalar(mul, 0.5623413251903491)\n",
      "            permute: \"f32[2, 3, 10, 10]\" = torch.ops.aten.permute.default(mul, [0, 1, 3, 2])\n",
      "            mul_2: \"f32[2, 3, 10, 10]\" = torch.ops.aten.mul.Scalar(permute, 0.5623413251903491);  permute = None\n",
      "            expand: \"f32[2, 3, 10, 10]\" = torch.ops.aten.expand.default(mul_1, [2, 3, 10, 10]);  mul_1 = None\n",
      "            view: \"f32[6, 10, 10]\" = torch.ops.aten.view.default(expand, [6, 10, 10]);  expand = None\n",
      "            expand_1: \"f32[2, 3, 10, 10]\" = torch.ops.aten.expand.default(mul_2, [2, 3, 10, 10]);  mul_2 = None\n",
      "            view_1: \"f32[6, 10, 10]\" = torch.ops.aten.view.default(expand_1, [6, 10, 10]);  expand_1 = None\n",
      "            bmm: \"f32[6, 10, 10]\" = torch.ops.aten.bmm.default(view, view_1);  view = view_1 = None\n",
      "            view_2: \"f32[2, 3, 10, 10]\" = torch.ops.aten.view.default(bmm, [2, 3, 10, 10]);  bmm = None\n",
      "            _softmax: \"f32[2, 3, 10, 10]\" = torch.ops.aten._softmax.default(view_2, -1, False)\n",
      "            eq: \"b8[2, 3, 10, 10]\" = torch.ops.aten.eq.Scalar(view_2, -inf);  view_2 = None\n",
      "            logical_not: \"b8[2, 3, 10, 10]\" = torch.ops.aten.logical_not.default(eq);  eq = None\n",
      "            any_1: \"b8[2, 3, 10, 1]\" = torch.ops.aten.any.dim(logical_not, -1, True);  logical_not = None\n",
      "            logical_not_1: \"b8[2, 3, 10, 1]\" = torch.ops.aten.logical_not.default(any_1);  any_1 = None\n",
      "            full_like: \"f32[2, 3, 10, 10]\" = torch.ops.aten.full_like.default(_softmax, 0, pin_memory = False, memory_format = torch.preserve_format)\n",
      "            where: \"f32[2, 3, 10, 10]\" = torch.ops.aten.where.self(logical_not_1, full_like, _softmax);  logical_not_1 = full_like = _softmax = None\n",
      "            expand_2: \"f32[2, 3, 10, 10]\" = torch.ops.aten.expand.default(where, [2, 3, 10, 10]);  where = None\n",
      "            view_3: \"f32[6, 10, 10]\" = torch.ops.aten.view.default(expand_2, [6, 10, 10]);  expand_2 = None\n",
      "            expand_3: \"f32[2, 3, 10, 10]\" = torch.ops.aten.expand.default(mul, [2, 3, 10, 10]);  mul = None\n",
      "            view_4: \"f32[6, 10, 10]\" = torch.ops.aten.view.default(expand_3, [6, 10, 10]);  expand_3 = None\n",
      "            bmm_1: \"f32[6, 10, 10]\" = torch.ops.aten.bmm.default(view_3, view_4);  view_3 = view_4 = None\n",
      "            view_5: \"f32[2, 3, 10, 10]\" = torch.ops.aten.view.default(bmm_1, [2, 3, 10, 10]);  bmm_1 = None\n",
      "            permute_1: \"f32[10, 2, 3, 10]\" = torch.ops.aten.permute.default(view_5, [2, 0, 1, 3]);  view_5 = None\n",
      "            clone: \"f32[10, 2, 3, 10]\" = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None\n",
      "            permute_2: \"f32[2, 3, 10, 10]\" = torch.ops.aten.permute.default(clone, [1, 2, 0, 3]);  clone = None\n",
      "            return (permute_2,)\n",
      "            \n",
      "Graph signature: \n",
      "    # inputs\n",
      "    p_weight: PARAMETER target='weight'\n",
      "    x: USER_INPUT\n",
      "    y: USER_INPUT\n",
      "    \n",
      "    # outputs\n",
      "    permute_2: USER_OUTPUT\n",
      "    \n",
      "Range constraints: {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decomposed = exported_program.run_decompositions()\n",
    "print(decomposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf85f2",
   "metadata": {},
   "source": [
    "## Translation to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbef4cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "x input_kind: InputKind.USER_INPUT persistent: None\n",
      "y input_kind: InputKind.USER_INPUT persistent: None\n",
      "p_weight input_kind: InputKind.PARAMETER persistent: None\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "[torch.onnx] Export report has been saved to 'onnx_export_2026-01-05_12-59-01-776388_success.md'.\n",
      "ONNXProgram(\n",
      "    model=\n",
      "        <\n",
      "            ir_version=10,\n",
      "            opset_imports={'': 23},\n",
      "            producer_name='pytorch',\n",
      "            producer_version='2.10.0.dev20251028+cpu',\n",
      "            domain=None,\n",
      "            model_version=None,\n",
      "        >\n",
      "        graph(\n",
      "            name=main_graph,\n",
      "            inputs=(\n",
      "                %\"x\"<FLOAT,[2,3,10,10]>,\n",
      "                %\"y\"<FLOAT,[2,3,10,10]>\n",
      "            ),\n",
      "            outputs=(\n",
      "                %\"scaled_dot_product_attention\"<FLOAT,[2,3,10,10]>\n",
      "            ),\n",
      "            initializers=(\n",
      "                %\"weight\"<FLOAT,[1,3,1,10]>{TorchTensor(...)}\n",
      "            ),\n",
      "        ) {\n",
      "            0 |  # node_sin\n",
      "                 %\"sin\"<FLOAT,[2,3,10,10]> ⬅️ ::Sin(%\"x\")\n",
      "            1 |  # node_add\n",
      "                 %\"add\"<FLOAT,[2,3,10,10]> ⬅️ ::Add(%\"sin\", %\"y\")\n",
      "            2 |  # node_mul\n",
      "                 %\"mul\"<FLOAT,[2,3,10,10]> ⬅️ ::Mul(%\"add\", %\"weight\"{...})\n",
      "            3 |  # node_scaled_dot_product_attention\n",
      "                 %\"scaled_dot_product_attention\"<FLOAT,[2,3,10,10]> ⬅️ ::Attention(%\"mul\", %\"mul\", %\"mul\") {qk_matmul_output_mode=0, softcap=0.0, is_causal=0}\n",
      "            return %\"scaled_dot_product_attention\"<FLOAT,[2,3,10,10]>\n",
      "        }\n",
      "\n",
      "\n",
      "    ,\n",
      "    exported_program=\n",
      "        ExportedProgram:\n",
      "            class GraphModule(torch.nn.Module):\n",
      "                def forward(self, p_weight: \"f32[1, 3, 1, 10]\", x: \"f32[2, 3, 10, 10]\", y: \"f32[2, 3, 10, 10]\"):\n",
      "                     # File: /tmp/ipykernel_246584/911034161.py:10 in forward, code: a = torch.sin(x)\n",
      "                    sin: \"f32[2, 3, 10, 10]\" = torch.ops.aten.sin.default(x);  x = None\n",
      "            \n",
      "                     # File: /tmp/ipykernel_246584/911034161.py:11 in forward, code: a.add_(y)\n",
      "                    add: \"f32[2, 3, 10, 10]\" = torch.ops.aten.add.Tensor(sin, y);  sin = y = None\n",
      "            \n",
      "                     # File: /tmp/ipykernel_246584/911034161.py:12 in forward, code: b = a * self.weight\n",
      "                    mul: \"f32[2, 3, 10, 10]\" = torch.ops.aten.mul.Tensor(add, p_weight);  add = p_weight = None\n",
      "            \n",
      "                     # File: /tmp/ipykernel_246584/911034161.py:13 in forward, code: return torch.nn.functional.scaled_dot_product_attention(b, b, b)\n",
      "                    scaled_dot_product_attention: \"f32[2, 3, 10, 10]\" = torch.ops.aten.scaled_dot_product_attention.default(mul, mul, mul);  mul = None\n",
      "                    return (scaled_dot_product_attention,)\n",
      "            \n",
      "        Graph signature: \n",
      "            # inputs\n",
      "            p_weight: PARAMETER target='weight'\n",
      "            x: USER_INPUT\n",
      "            y: USER_INPUT\n",
      "    \n",
      "            # outputs\n",
      "            scaled_dot_product_attention: USER_OUTPUT\n",
      "    \n",
      "        Range constraints: {}\n",
      "\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onnx_program = torch.onnx.export(exported_program, report=True, opset_version=23)\n",
    "print(onnx_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "877d62e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_program.save(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8f18b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading extensions...\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767644749.496075  248247 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\n",
      "Loaded 9 adapters:\n",
      " - TFLite adapter (Flatbuffer)\n",
      " - TFLite adapter (MLIR)\n",
      " - TF adapter (MLIR)\n",
      " - TF adapter (direct)\n",
      " - GraphDef adapter\n",
      " - Pytorch adapter (exported program)\n",
      " - MLIR adapter\n",
      " - ONNX adapter\n",
      " - JSON adapter\n",
      "\n",
      "Starting Model Explorer server at:\n",
      "http://localhost:8080/?data=%7B%22models%22%3A%20%5B%7B%22url%22%3A%20%22/home/justinchu/dev/talk-torch-onnx-apis-architecture/src/model.onnx%22%7D%5D%7D\n",
      "\n",
      "Press Ctrl+C to stop.\n",
      "gio: http://localhost:8080/?data=%7B%22models%22%3A%20%5B%7B%22url%22%3A%20%22/home/justinchu/dev/talk-torch-onnx-apis-architecture/src/model.onnx%22%7D%5D%7D: Operation not supported\n",
      "Stopping server...\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!onnxvis model.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f8ed9e",
   "metadata": {},
   "source": [
    "## Model in `onnx_program.model` is an onnx_ir.Model\n",
    "\n",
    "- You can run any ONNX->ONNX transformation on it.\n",
    "- The exporter by default runs ONNX Script pattern replacement and whole graph optimization. These are robust, in-memory graph passes the team has created\n",
    "- Low memory consumption by sharing tensor data with the PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39ab8304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 4 nodes\n",
      "All initializers:\n",
      "  %\"weight\"<FLOAT,[1,3,1,10]>{TorchTensor(...)}\n"
     ]
    }
   ],
   "source": [
    "# Explore the IR model\n",
    "\n",
    "model = onnx_program.model\n",
    "print(\"Model has\", len(model.graph), \"nodes\")\n",
    "\n",
    "print(\"All initializers:\")\n",
    "for init in model.graph.initializers.values():\n",
    "    print(\" \", init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eb16c985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(model.graph.initializers[\"weight\"].const_value.raw is mod.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e0fe48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TorchTensor<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">FLOAT</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"font-weight: bold\">&gt;(</span>Parameter containing: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2867</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3785</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.5398</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.1127</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3554</span>,  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2557</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6805</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2783</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7972</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3939</span><span style=\"font-weight: bold\">]]</span>,  <span style=\"font-weight: bold\">[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0281</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2927</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8188</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5172</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6188</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2352</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0469</span>, \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1091</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4472</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5587</span><span style=\"font-weight: bold\">]]</span>,  <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.5244</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2047</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1691</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.4470</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1873</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0693</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5142</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.9798</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2267</span>,  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3867</span><span style=\"font-weight: bold\">]]]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">requires_grad</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'weight'</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "Min: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.1127448081970215</span>, Max: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.4470179080963135</span>, NaN count: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, Inf count: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "Sparsity <span style=\"font-weight: bold\">(</span>abs&lt;<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-06</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>\n",
       "Histogram:\n",
       "       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> ┼                                        ╭╮\n",
       "       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> ┤                                        ││\n",
       "       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> ┤                                        ││\n",
       "       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> ┤                 ╭╮       ╭╮ ╭╮╭╮       ││╭╮ ╭╮\n",
       "       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> ┤                 ││       ││ ││││       ││││ ││\n",
       "       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> ┼╮                │╰╮  ╭╮  ││╭╯││╰╮  ╭─╮ │╰╯│ │╰╮╭╮╭╮    ╭─╮   ╭╮              ╭\n",
       "       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> ┤│                │ │  ││  │││ ││ │  │ │ │  │ │ │││││    │ │   ││              │\n",
       "       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> ┤│                │ │  ││  │││ ││ │  │ │ │  │ │ │││││    │ │   ││              │\n",
       "       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> ┤╰────────────────╯ ╰──╯╰──╯╰╯ ╰╯ ╰──╯ ╰─╯  ╰─╯ ╰╯╰╯╰────╯ ╰───╯╰──────────────╯\n",
       "    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.1127</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.6568</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.2008</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6878</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2318</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2811</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6801</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1361</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5351</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.9910</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.4470</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TorchTensor\u001b[1m<\u001b[0m\u001b[1;95mFLOAT\u001b[0m\u001b[39m,\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[39m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1m>\u001b[0m\u001b[1m(\u001b[0mParameter containing: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.2867\u001b[0m, \u001b[1;36m-0.3785\u001b[0m, \u001b[1;36m-0.5398\u001b[0m, \u001b[1;36m-2.1127\u001b[0m,  \u001b[1;36m0.3554\u001b[0m,  \n",
       "\u001b[1;36m0.2557\u001b[0m,  \u001b[1;36m0.6805\u001b[0m, \u001b[1;36m0.2783\u001b[0m, \u001b[1;36m-0.7972\u001b[0m, \u001b[1;36m-0.3939\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,  \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0281\u001b[0m,  \u001b[1;36m0.2927\u001b[0m,  \u001b[1;36m0.8188\u001b[0m,  \u001b[1;36m1.5172\u001b[0m,  \u001b[1;36m0.6188\u001b[0m, \u001b[1;36m-0.2352\u001b[0m, \u001b[1;36m-1.0469\u001b[0m, \n",
       "\u001b[1;36m0.1091\u001b[0m, \u001b[1;36m-0.4472\u001b[0m,  \u001b[1;36m0.5587\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,  \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.5244\u001b[0m,  \u001b[1;36m1.2047\u001b[0m,  \u001b[1;36m1.1691\u001b[0m,  \u001b[1;36m2.4470\u001b[0m, \u001b[1;36m-0.1873\u001b[0m, \u001b[1;36m-1.0693\u001b[0m,  \u001b[1;36m0.5142\u001b[0m, \u001b[1;36m-0.9798\u001b[0m,  \u001b[1;36m0.2267\u001b[0m,  \n",
       "\u001b[1;36m0.3867\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mrequires_grad\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mname\u001b[0m=\u001b[32m'weight'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "Min: \u001b[1;36m-2.1127448081970215\u001b[0m, Max: \u001b[1;36m2.4470179080963135\u001b[0m, NaN count: \u001b[1;36m0\u001b[0m, Inf count: \u001b[1;36m0\u001b[0m\n",
       "Sparsity \u001b[1m(\u001b[0mabs<\u001b[1;36m1e-06\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m0.00\u001b[0m\n",
       "Histogram:\n",
       "       \u001b[1;36m3\u001b[0m ┼                                        ╭╮\n",
       "       \u001b[1;36m3\u001b[0m ┤                                        ││\n",
       "       \u001b[1;36m2\u001b[0m ┤                                        ││\n",
       "       \u001b[1;36m2\u001b[0m ┤                 ╭╮       ╭╮ ╭╮╭╮       ││╭╮ ╭╮\n",
       "       \u001b[1;36m2\u001b[0m ┤                 ││       ││ ││││       ││││ ││\n",
       "       \u001b[1;36m1\u001b[0m ┼╮                │╰╮  ╭╮  ││╭╯││╰╮  ╭─╮ │╰╯│ │╰╮╭╮╭╮    ╭─╮   ╭╮              ╭\n",
       "       \u001b[1;36m1\u001b[0m ┤│                │ │  ││  │││ ││ │  │ │ │  │ │ │││││    │ │   ││              │\n",
       "       \u001b[1;36m0\u001b[0m ┤│                │ │  ││  │││ ││ │  │ │ │  │ │ │││││    │ │   ││              │\n",
       "       \u001b[1;36m0\u001b[0m ┤╰────────────────╯ ╰──╯╰──╯╰╯ ╰╯ ╰──╯ ╰─╯  ╰─╯ ╰╯╰╯╰────╯ ╰───╯╰──────────────╯\n",
       "    \u001b[1;36m-2.1127\u001b[0m  \u001b[1;36m-1.6568\u001b[0m  \u001b[1;36m-1.2008\u001b[0m  \u001b[1;36m-0.6878\u001b[0m  \u001b[1;36m-0.2318\u001b[0m  \u001b[1;36m0.2811\u001b[0m  \u001b[1;36m0.6801\u001b[0m  \u001b[1;36m1.1361\u001b[0m  \u001b[1;36m1.5351\u001b[0m  \u001b[1;36m1.9910\u001b[0m  \u001b[1;36m2.4470\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.graph.initializers[\"weight\"].const_value.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9beb92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All users of the initializer: (Usage(node=Node(name='node_mul', domain='', op_type='Mul', inputs=(SymbolicTensor(name='add', type=Tensor(FLOAT), shape=Shape([2, 3, 10, 10]), producer='node_add', index=0), SymbolicTensor(name='weight', type=Tensor(FLOAT), shape=Shape([2, 3, 10, 10]), const_value={TorchTensor(...)})), attributes={}, overload='', outputs=(SymbolicTensor(name='mul', type=Tensor(FLOAT), shape=Shape([2, 3, 10, 10]), producer='node_mul', index=0),), version=23, doc_string=None), idx=1),)\n"
     ]
    }
   ],
   "source": [
    "print(\"All users of the initializer:\", model.graph.initializers[\"weight\"].uses())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccacb259",
   "metadata": {},
   "source": [
    "## Verify model outputs\n",
    "\n",
    "https://github.com/justinchuby/model-explorer-onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40e4b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.onnx.verification import verify_onnx_program\n",
    "\n",
    "from model_explorer_onnx.torch_utils import save_node_data_from_verification_info\n",
    "\n",
    "verification_infos = verify_onnx_program(onnx_program, compare_intermediates=True)\n",
    "\n",
    "# Produce node data for Model Explorer for visualization\n",
    "save_node_data_from_verification_info(\n",
    "    verification_infos, onnx_program.model, model_name=\"model\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86452cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading extensions...\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767645930.860103  250391 port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\n",
      "Loaded 9 adapters:\n",
      " - TFLite adapter (Flatbuffer)\n",
      " - TFLite adapter (MLIR)\n",
      " - TF adapter (MLIR)\n",
      " - TF adapter (direct)\n",
      " - GraphDef adapter\n",
      " - Pytorch adapter (exported program)\n",
      " - MLIR adapter\n",
      " - ONNX adapter\n",
      " - JSON adapter\n",
      "\n",
      "Starting Model Explorer server at:\n",
      "http://localhost:8080/?data=%7B%22models%22%3A%20%5B%7B%22url%22%3A%20%22/home/justinchu/dev/talk-torch-onnx-apis-architecture/src/model.onnx%22%7D%5D%2C%20%22nodeData%22%3A%20%5B%22/home/justinchu/dev/talk-torch-onnx-apis-architecture/src/model_max_abs_diff.json%22%2C%20%22/home/justinchu/dev/talk-torch-onnx-apis-architecture/src/model_max_rel_diff.json%22%5D%2C%20%22nodeDataTargets%22%3A%20%5B%22%22%2C%20%22%22%5D%7D\n",
      "\n",
      "Press Ctrl+C to stop.\n",
      "gio: http://localhost:8080/?data=%7B%22models%22%3A%20%5B%7B%22url%22%3A%20%22/home/justinchu/dev/talk-torch-onnx-apis-architecture/src/model.onnx%22%7D%5D%2C%20%22nodeData%22%3A%20%5B%22/home/justinchu/dev/talk-torch-onnx-apis-architecture/src/model_max_abs_diff.json%22%2C%20%22/home/justinchu/dev/talk-torch-onnx-apis-architecture/src/model_max_rel_diff.json%22%5D%2C%20%22nodeDataTargets%22%3A%20%5B%22%22%2C%20%22%22%5D%7D: Operation not supported\n",
      "Stopping server...\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!onnxvis model.onnx --node_data_paths=model_max_abs_diff.json,model_max_rel_diff.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97327cfa",
   "metadata": {},
   "source": [
    "## Multiple ways to represent dynamic shapes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9b77e977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: Using strings (recommended)\n",
      "[torch.onnx] Obtain model graph for `Mod()` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `Mod()` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "x input_kind: InputKind.USER_INPUT persistent: None\n",
      "y input_kind: InputKind.USER_INPUT persistent: None\n",
      "p_weight input_kind: InputKind.PARAMETER persistent: None\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "ONNXProgram(\n",
      "    model=\n",
      "        <\n",
      "            ir_version=10,\n",
      "            opset_imports={'': 23},\n",
      "            producer_name='pytorch',\n",
      "            producer_version='2.10.0.dev20251028+cpu',\n",
      "            domain=None,\n",
      "            model_version=None,\n",
      "        >\n",
      "        graph(\n",
      "            name=main_graph,\n",
      "            inputs=(\n",
      "                %\"x\"<FLOAT,[batch,3,seq_len,10]>,\n",
      "                %\"y\"<FLOAT,[batch,3,seq_len,10]>\n",
      "            ),\n",
      "            outputs=(\n",
      "                %\"scaled_dot_product_attention\"<FLOAT,[batch,3,seq_len,10]>\n",
      "            ),\n",
      "            initializers=(\n",
      "                %\"weight\"<FLOAT,[1,3,1,10]>{TorchTensor(...)}\n",
      "            ),\n",
      "        ) {\n",
      "            0 |  # node_sin\n",
      "                 %\"sin\"<FLOAT,[batch,3,seq_len,10]> ⬅️ ::Sin(%\"x\")\n",
      "            1 |  # node_add_15\n",
      "                 %\"add_15\"<FLOAT,[batch,3,seq_len,10]> ⬅️ ::Add(%\"sin\", %\"y\")\n",
      "            2 |  # node_mul_16\n",
      "                 %\"mul_16\"<FLOAT,[batch,3,seq_len,10]> ⬅️ ::Mul(%\"add_15\", %\"weight\"{...})\n",
      "            3 |  # node_scaled_dot_product_attention\n",
      "                 %\"scaled_dot_product_attention\"<FLOAT,[batch,3,seq_len,10]> ⬅️ ::Attention(%\"mul_16\", %\"mul_16\", %\"mul_16\") {qk_matmul_output_mode=0, softcap=0.0, is_causal=0}\n",
      "            return %\"scaled_dot_product_attention\"<FLOAT,[batch,3,seq_len,10]>\n",
      "        }\n",
      "\n",
      "\n",
      "    ,\n",
      "    exported_program=\n",
      "        ExportedProgram:\n",
      "            class GraphModule(torch.nn.Module):\n",
      "                def forward(self, p_weight: \"f32[1, 3, 1, 10]\", x: \"f32[s17, 3, s48, 10]\", y: \"f32[s17, 3, s48, 10]\"):\n",
      "                     # \n",
      "                    sym_size_int_4: \"Sym(s17)\" = torch.ops.aten.sym_size.int(x, 0)\n",
      "                    sym_size_int_5: \"Sym(s48)\" = torch.ops.aten.sym_size.int(x, 2)\n",
      "                    sym_size_int_6: \"Sym(s17)\" = torch.ops.aten.sym_size.int(y, 0)\n",
      "                    sym_size_int_7: \"Sym(s48)\" = torch.ops.aten.sym_size.int(y, 2)\n",
      "                    eq_24: \"Sym(True)\" = sym_size_int_5 == sym_size_int_7;  sym_size_int_5 = eq_24 = None\n",
      "                    eq_25: \"Sym(True)\" = sym_size_int_4 == sym_size_int_6;  sym_size_int_4 = eq_25 = None\n",
      "            \n",
      "                     # File: /tmp/ipykernel_246584/911034161.py:10 in forward, code: a = torch.sin(x)\n",
      "                    sin: \"f32[s17, 3, s48, 10]\" = torch.ops.aten.sin.default(x);  x = None\n",
      "            \n",
      "                     # \n",
      "                    eq_4: \"Sym(True)\" = sym_size_int_7 == sym_size_int_7;  sym_size_int_7 = eq_4 = None\n",
      "                    eq_5: \"Sym(True)\" = sym_size_int_6 == sym_size_int_6;  sym_size_int_6 = eq_5 = None\n",
      "            \n",
      "                     # File: /tmp/ipykernel_246584/911034161.py:11 in forward, code: a.add_(y)\n",
      "                    add_15: \"f32[s17, 3, s48, 10]\" = torch.ops.aten.add.Tensor(sin, y);  sin = y = None\n",
      "            \n",
      "                     # File: /tmp/ipykernel_246584/911034161.py:12 in forward, code: b = a * self.weight\n",
      "                    mul_16: \"f32[s17, 3, s48, 10]\" = torch.ops.aten.mul.Tensor(add_15, p_weight);  add_15 = p_weight = None\n",
      "            \n",
      "                     # File: /tmp/ipykernel_246584/911034161.py:13 in forward, code: return torch.nn.functional.scaled_dot_product_attention(b, b, b)\n",
      "                    scaled_dot_product_attention: \"f32[s17, 3, s48, 10]\" = torch.ops.aten.scaled_dot_product_attention.default(mul_16, mul_16, mul_16);  mul_16 = None\n",
      "                    return (scaled_dot_product_attention,)\n",
      "            \n",
      "        Graph signature: \n",
      "            # inputs\n",
      "            p_weight: PARAMETER target='weight'\n",
      "            x: USER_INPUT\n",
      "            y: USER_INPUT\n",
      "    \n",
      "            # outputs\n",
      "            scaled_dot_product_attention: USER_OUTPUT\n",
      "    \n",
      "        Range constraints: {s17: VR[0, int_oo], s48: VR[0, int_oo]}\n",
      "\n",
      ")\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Method 2: Using ShapesCollection\n",
      "[torch.onnx] Obtain model graph for `Mod()` with `torch.export.export(..., strict=False)`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justinchu/anaconda3/envs/onnx/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_onnx_program.py:460: UserWarning: # The axis name: batch will not be used, since it shares the same shape constraints with another axis: batch.\n",
      "  rename_mapping = _dynamic_shapes.create_rename_mapping(\n",
      "/home/justinchu/anaconda3/envs/onnx/lib/python3.13/site-packages/torch/onnx/_internal/exporter/_onnx_program.py:460: UserWarning: # The axis name: seq_len will not be used, since it shares the same shape constraints with another axis: seq_len.\n",
      "  rename_mapping = _dynamic_shapes.create_rename_mapping(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `Mod()` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "x input_kind: InputKind.USER_INPUT persistent: None\n",
      "y input_kind: InputKind.USER_INPUT persistent: None\n",
      "p_weight input_kind: InputKind.PARAMETER persistent: None\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "ONNXProgram(\n",
      "    model=\n",
      "        <\n",
      "            ir_version=10,\n",
      "            opset_imports={'': 23},\n",
      "            producer_name='pytorch',\n",
      "            producer_version='2.10.0.dev20251028+cpu',\n",
      "            domain=None,\n",
      "            model_version=None,\n",
      "        >\n",
      "        graph(\n",
      "            name=main_graph,\n",
      "            inputs=(\n",
      "                %\"x\"<FLOAT,[s17,3,s48,10]>,\n",
      "                %\"y\"<FLOAT,[s17,3,s48,10]>\n",
      "            ),\n",
      "            outputs=(\n",
      "                %\"scaled_dot_product_attention\"<FLOAT,[s17,3,s48,10]>\n",
      "            ),\n",
      "            initializers=(\n",
      "                %\"weight\"<FLOAT,[1,3,1,10]>{TorchTensor(...)}\n",
      "            ),\n",
      "        ) {\n",
      "            0 |  # node_sin\n",
      "                 %\"sin\"<FLOAT,[s17,3,s48,10]> ⬅️ ::Sin(%\"x\")\n",
      "            1 |  # node_add_15\n",
      "                 %\"add_15\"<FLOAT,[s17,3,s48,10]> ⬅️ ::Add(%\"sin\", %\"y\")\n",
      "            2 |  # node_mul_16\n",
      "                 %\"mul_16\"<FLOAT,[s17,3,s48,10]> ⬅️ ::Mul(%\"add_15\", %\"weight\"{...})\n",
      "            3 |  # node_scaled_dot_product_attention\n",
      "                 %\"scaled_dot_product_attention\"<FLOAT,[s17,3,s48,10]> ⬅️ ::Attention(%\"mul_16\", %\"mul_16\", %\"mul_16\") {qk_matmul_output_mode=0, softcap=0.0, is_causal=0}\n",
      "            return %\"scaled_dot_product_attention\"<FLOAT,[s17,3,s48,10]>\n",
      "        }\n",
      "\n",
      "\n",
      "    ,\n",
      "    exported_program=\n",
      "        ExportedProgram:\n",
      "            class GraphModule(torch.nn.Module):\n",
      "                def forward(self, p_weight: \"f32[1, 3, 1, 10]\", x: \"f32[s17, 3, s48, 10]\", y: \"f32[s17, 3, s48, 10]\"):\n",
      "                     # \n",
      "                    sym_size_int_4: \"Sym(s17)\" = torch.ops.aten.sym_size.int(x, 0)\n",
      "                    sym_size_int_5: \"Sym(s48)\" = torch.ops.aten.sym_size.int(x, 2)\n",
      "                    sym_size_int_6: \"Sym(s17)\" = torch.ops.aten.sym_size.int(y, 0)\n",
      "                    sym_size_int_7: \"Sym(s48)\" = torch.ops.aten.sym_size.int(y, 2)\n",
      "                    eq_24: \"Sym(True)\" = sym_size_int_5 == sym_size_int_7;  sym_size_int_5 = eq_24 = None\n",
      "                    eq_25: \"Sym(True)\" = sym_size_int_4 == sym_size_int_6;  sym_size_int_4 = eq_25 = None\n",
      "            \n",
      "                     # File: /tmp/ipykernel_246584/911034161.py:10 in forward, code: a = torch.sin(x)\n",
      "                    sin: \"f32[s17, 3, s48, 10]\" = torch.ops.aten.sin.default(x);  x = None\n",
      "            \n",
      "                     # \n",
      "                    eq_4: \"Sym(True)\" = sym_size_int_7 == sym_size_int_7;  sym_size_int_7 = eq_4 = None\n",
      "                    eq_5: \"Sym(True)\" = sym_size_int_6 == sym_size_int_6;  sym_size_int_6 = eq_5 = None\n",
      "            \n",
      "                     # File: /tmp/ipykernel_246584/911034161.py:11 in forward, code: a.add_(y)\n",
      "                    add_15: \"f32[s17, 3, s48, 10]\" = torch.ops.aten.add.Tensor(sin, y);  sin = y = None\n",
      "            \n",
      "                     # File: /tmp/ipykernel_246584/911034161.py:12 in forward, code: b = a * self.weight\n",
      "                    mul_16: \"f32[s17, 3, s48, 10]\" = torch.ops.aten.mul.Tensor(add_15, p_weight);  add_15 = p_weight = None\n",
      "            \n",
      "                     # File: /tmp/ipykernel_246584/911034161.py:13 in forward, code: return torch.nn.functional.scaled_dot_product_attention(b, b, b)\n",
      "                    scaled_dot_product_attention: \"f32[s17, 3, s48, 10]\" = torch.ops.aten.scaled_dot_product_attention.default(mul_16, mul_16, mul_16);  mul_16 = None\n",
      "                    return (scaled_dot_product_attention,)\n",
      "            \n",
      "        Graph signature: \n",
      "            # inputs\n",
      "            p_weight: PARAMETER target='weight'\n",
      "            x: USER_INPUT\n",
      "            y: USER_INPUT\n",
      "    \n",
      "            # outputs\n",
      "            scaled_dot_product_attention: USER_OUTPUT\n",
      "    \n",
      "        Range constraints: {s17: VR[0, int_oo], s48: VR[0, int_oo]}\n",
      "\n",
      ")\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Method 3: Using AdditionalInputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0105 15:54:04.031000 246584 site-packages/torch/export/dynamic_shapes.py:919] Using None as a dynamic shape dimension is deprecated. Please use Dim.STATIC instead\n",
      "W0105 15:54:04.032000 246584 site-packages/torch/export/dynamic_shapes.py:919] Using None as a dynamic shape dimension is deprecated. Please use Dim.STATIC instead\n",
      "W0105 15:54:04.032000 246584 site-packages/torch/export/dynamic_shapes.py:919] Using None as a dynamic shape dimension is deprecated. Please use Dim.STATIC instead\n",
      "W0105 15:54:04.033000 246584 site-packages/torch/export/dynamic_shapes.py:919] Using None as a dynamic shape dimension is deprecated. Please use Dim.STATIC instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `Mod()` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `Mod()` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "x input_kind: InputKind.USER_INPUT persistent: None\n",
      "y input_kind: InputKind.USER_INPUT persistent: None\n",
      "p_weight input_kind: InputKind.PARAMETER persistent: None\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "ONNXProgram(\n",
      "    model=\n",
      "        <\n",
      "            ir_version=10,\n",
      "            opset_imports={'': 23},\n",
      "            producer_name='pytorch',\n",
      "            producer_version='2.10.0.dev20251028+cpu',\n",
      "            domain=None,\n",
      "            model_version=None,\n",
      "        >\n",
      "        graph(\n",
      "            name=main_graph,\n",
      "            inputs=(\n",
      "                %\"x\"<FLOAT,[s17,3,s48,10]>,\n",
      "                %\"y\"<FLOAT,[s17,3,s48,10]>\n",
      "            ),\n",
      "            outputs=(\n",
      "                %\"scaled_dot_product_attention\"<FLOAT,[s17,3,s48,10]>\n",
      "            ),\n",
      "            initializers=(\n",
      "                %\"weight\"<FLOAT,[1,3,1,10]>{TorchTensor(...)}\n",
      "            ),\n",
      "        ) {\n",
      "            0 |  # node_sin\n",
      "                 %\"sin\"<FLOAT,[s17,3,s48,10]> ⬅️ ::Sin(%\"x\")\n",
      "            1 |  # node_add_15\n",
      "                 %\"add_15\"<FLOAT,[s17,3,s48,10]> ⬅️ ::Add(%\"sin\", %\"y\")\n",
      "            2 |  # node_mul_16\n",
      "                 %\"mul_16\"<FLOAT,[s17,3,s48,10]> ⬅️ ::Mul(%\"add_15\", %\"weight\"{...})\n",
      "            3 |  # node_scaled_dot_product_attention\n",
      "                 %\"scaled_dot_product_attention\"<FLOAT,[s17,3,s48,10]> ⬅️ ::Attention(%\"mul_16\", %\"mul_16\", %\"mul_16\") {qk_matmul_output_mode=0, softcap=0.0, is_causal=0}\n",
      "            return %\"scaled_dot_product_attention\"<FLOAT,[s17,3,s48,10]>\n",
      "        }\n",
      "\n",
      "\n",
      "    ,\n",
      "    exported_program=\n",
      "        ExportedProgram:\n",
      "            class GraphModule(torch.nn.Module):\n",
      "                def forward(self, p_weight: \"f32[1, 3, 1, 10]\", x: \"f32[s17, 3, s48, 10]\", y: \"f32[s17, 3, s48, 10]\"):\n",
      "                     # \n",
      "                    sym_size_int_4: \"Sym(s17)\" = torch.ops.aten.sym_size.int(x, 0)\n",
      "                    sym_size_int_5: \"Sym(s48)\" = torch.ops.aten.sym_size.int(x, 2)\n",
      "                    sym_size_int_6: \"Sym(s17)\" = torch.ops.aten.sym_size.int(y, 0)\n",
      "                    sym_size_int_7: \"Sym(s48)\" = torch.ops.aten.sym_size.int(y, 2)\n",
      "                    eq_24: \"Sym(True)\" = sym_size_int_5 == sym_size_int_7;  sym_size_int_5 = eq_24 = None\n",
      "                    eq_25: \"Sym(True)\" = sym_size_int_4 == sym_size_int_6;  sym_size_int_4 = eq_25 = None\n",
      "            \n",
      "                     # File: /tmp/ipykernel_246584/911034161.py:10 in forward, code: a = torch.sin(x)\n",
      "                    sin: \"f32[s17, 3, s48, 10]\" = torch.ops.aten.sin.default(x);  x = None\n",
      "            \n",
      "                     # \n",
      "                    eq_4: \"Sym(True)\" = sym_size_int_7 == sym_size_int_7;  sym_size_int_7 = eq_4 = None\n",
      "                    eq_5: \"Sym(True)\" = sym_size_int_6 == sym_size_int_6;  sym_size_int_6 = eq_5 = None\n",
      "            \n",
      "                     # File: /tmp/ipykernel_246584/911034161.py:11 in forward, code: a.add_(y)\n",
      "                    add_15: \"f32[s17, 3, s48, 10]\" = torch.ops.aten.add.Tensor(sin, y);  sin = y = None\n",
      "            \n",
      "                     # File: /tmp/ipykernel_246584/911034161.py:12 in forward, code: b = a * self.weight\n",
      "                    mul_16: \"f32[s17, 3, s48, 10]\" = torch.ops.aten.mul.Tensor(add_15, p_weight);  add_15 = p_weight = None\n",
      "            \n",
      "                     # File: /tmp/ipykernel_246584/911034161.py:13 in forward, code: return torch.nn.functional.scaled_dot_product_attention(b, b, b)\n",
      "                    scaled_dot_product_attention: \"f32[s17, 3, s48, 10]\" = torch.ops.aten.scaled_dot_product_attention.default(mul_16, mul_16, mul_16);  mul_16 = None\n",
      "                    return (scaled_dot_product_attention,)\n",
      "            \n",
      "        Graph signature: \n",
      "            # inputs\n",
      "            p_weight: PARAMETER target='weight'\n",
      "            x: USER_INPUT\n",
      "            y: USER_INPUT\n",
      "    \n",
      "            # outputs\n",
      "            scaled_dot_product_attention: USER_OUTPUT\n",
      "    \n",
      "        Range constraints: {s17: VR[0, int_oo], s48: VR[0, int_oo]}\n",
      "\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate different ways to express dynamic shapes in PyTorch export and ONNX export\n",
    "from torch.export import Dim, ShapesCollection\n",
    "\n",
    "# Method 1: Using strings\n",
    "print(\"Method 1: Using strings (recommended)\")\n",
    "simple_dynamic_shapes = (\n",
    "    {0: \"batch\", 2: \"seq_len\"},\n",
    "    {0: \"batch\", 2: \"seq_len\"}\n",
    ")\n",
    "onnx_prog_simple = torch.onnx.export(\n",
    "    mod,\n",
    "    args=example_args,\n",
    "    dynamic_shapes=simple_dynamic_shapes,\n",
    "    opset_version=23,\n",
    ")\n",
    "print(onnx_prog_simple)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Method 2: Using ShapesCollection\n",
    "shapes_collection = ShapesCollection()\n",
    "shapes_collection[example_args[0]] = {0: Dim.DYNAMIC, 2: Dim.DYNAMIC}\n",
    "shapes_collection[example_args[1]] = {0: Dim.DYNAMIC, 2: Dim.DYNAMIC}\n",
    "\n",
    "print(\"Method 2: Using ShapesCollection\")\n",
    "onnx_prog_shapes = torch.onnx.export(\n",
    "    mod,\n",
    "    args=example_args,\n",
    "    dynamic_shapes=shapes_collection,\n",
    "    opset_version=23,\n",
    ")\n",
    "print(onnx_prog_shapes)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Method 3: Using AdditionalInputs\n",
    "ai = torch.export.AdditionalInputs()\n",
    "example_args_1 = (torch.randn(2, 3, 10, 10), torch.randn(2, 3, 10, 10))\n",
    "example_args_2 = (torch.randn(4, 3, 2, 10), torch.randn(4, 3, 2, 10))\n",
    "ai.add(example_args_1)\n",
    "ai.add(example_args_2)\n",
    "\n",
    "print(\"Method 3: Using AdditionalInputs\")\n",
    "onnx_prog_shapes = torch.onnx.export(\n",
    "    mod,\n",
    "    args=example_args,\n",
    "    dynamic_shapes=ai,\n",
    "    opset_version=23,\n",
    ")\n",
    "print(onnx_prog_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb3199",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
